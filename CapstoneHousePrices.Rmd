---
title: "Capstone House Prices"
subtitle: 'HarvardX (edX) Data Science Professional Certificate: PH125.9x Capstone'
author: "Jonathan Ostler"
date: "`r format(Sys.Date())`"
output: 
  pdf_document: 
    df_print: kable
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: yes
  html_document: default
header-includes:
  \usepackage{float}
  \floatplacement{figure}{H}
  \floatplacement{table}{H}
include-before: '`\newpage{}`{=latex}'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-global-env , eval=TRUE, echo=FALSE}

#Ensures all the data created from the r script is available to the Rmd file
#Process - 
  # 1 - Run the script
  # 2 - Save the .Rmd file
  # 3 - Click on the Knit button/icon at the top of the IDE
  # 4 - This is the "pull" method
  # 5 - Alternatively set eval = FALSE for this chunk and "push" from console ====> #  rmarkdown::render("CapstoneHousePrices.Rmd")

load(file = "CapstoneHousePrices.RData")

```


\newpage
# Abstract
Data science has become the super-power behind the success of many businesses of today. These companies need and use data science approaches like machine learning to extract patterns, trends and other actionable information from large data sets  [$\color{blue}{\text{(Galwey, 2014)}}$](#references). In turn, companies use these findings to improve performance, grow and, perhaps most importantly, to provide better products and services to consumers [$\color{blue}{\text{(Irizarry, 2020)}}$](#references).

Not surprisingly, prediction and recommendation systems are increasingly used by businesses to provide personalized insights to consumers. Often companies make competitions out of the challenge to stimulate ideas and of course produce a better, workable and dependable model.

One of the most popular competitions on [$\color{blue}{\text{kaggle}}$](#references) is the "House Prices: Advanced Regression Techniques" competition. This competition was the inspiration for this project and the subject of this report.

Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But the dataset used here proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, U.S.A, the aim was to predict the Sale Prices with the highest possible accuracy. 

The Ames Housing dataset (covering sales during the early 2000s) was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. 

This report details some of the possible techniques used to predict the Sale Price and how they were built up from first principles.

The kaggle challenge used the standard error loss function, Root Mean Squared Errors (**RMSE**), based on the logarithms of the predicted and observed sale prices: 

$$RMSE = \sqrt{\frac{1}{N}\sum_{i}^{N}(log(\hat{y}_{i})-log(y_{i}))^2}$$
with the winner being chosen based on the lowest RMSE value. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)

This report shows the evolution of the RMSE value from an initial simple solution to the ultimate solution as more complex models are created.   

Since the ultimate sale price in the validation data was not provided, this report based its final RMSE value on a sample of the initial training data for validation/verification.

The GitHub repository for this report can be found at : [$\color{blue}{\text{https://github.com/ostlerjo/CapstoneHousePrices.git}}$](https://github.com/ostlerjo/CapstoneHousePrices.git)






\newpage
# Introduction
## Data Used
The original data comes from the publication Dean De Cock "Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project", Journal of Statistics Education, Volume 19, Number 3 (2011). 

The data used in this report was obtained from kaggle.com.    

## Libraries Used

```{r installing-libs, eval=TRUE, echo=FALSE, message=FALSE}

if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(Boruta)) install.packages("Boruta", repos = "http://cran.us.r-project.org")
if(!require(plyr)) install.packages("plyr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(FeatureHashing)) install.packages("FeatureHashing", repos = "http://cran.us.r-project.org")
if(!require(Matrix)) install.packages("Matrix", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(dummies)) install.packages("dummies", repos = "http://cran.us.r-project.org")
if(!require(Metrics)) install.packages("Metrics", repos = "http://cran.us.r-project.org")
if(!require(kernlab)) install.packages("kernlab", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(car)) install.packages("car", repos = "http://cran.us.r-project.org")
if(!require(lars)) install.packages("lars", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")

```


The following r libraries were used to generate the data, perform the analysis and create the algorithms detailed in this report:

```{r loading-libs, eval=TRUE, echo=TRUE,  message=FALSE}

library(caret)
library(data.table)
library(Boruta)
library(plyr)
library(dplyr)
library(pROC)
library(tidyverse)
library(FeatureHashing)
library(Matrix)
library(xgboost)
require(randomForest)
require(ggplot2)
library(stringr)
library(dummies)
library(Metrics)
library(kernlab)
library(corrplot)
library(car)
library(lars)
library(kableExtra)

```


\newpage
# Data Cleaning and Preparation

## Initial Data Cleaning

Due to the way r treats variable names, three columns were mutated to put a dummy X character in front e.g. 1stFlrSF became X1stFlrSF etc.

Secondly several columns had false NAs in them that would cause the code to give errors e.g. the column "Alley" had a set of NAs which in fact meant "No Alley". As a consequence, all of these types of values were explored and then updated to their correct meaning.

Finally a couple of numeric columns also had NAs when they should in fact mean 0. As a consequence, all of these types of values were updated to be 0 rather than NA.

## Data Partitioning
Two sets of data were available i.e. a train set and a test set. Since the data was originally going to be used in a competition to predict the Sale Prices, the test set provided did not have a Sale Price column. As a consequence for this project where predictive results needed to be reported, the train data set was used for training, testing and the final validation through partitioning.

Initially the train set was partitioned into a Modeling set and a Final Validation set. The split was 90% / 10% in favor of the Modeling set. The small percentage assigned to the Final Validation set was because the overall data set did not have so many values (around 1'500) and most of the data should be used for modeling. The Final Validation set was set aside and not used again until the Final RMSE calculation. 

Next the Modeling set was further partitioned into a Training set and a Testing set that would be used for analysis, model creation, training and initial validation. The split was 60 % / 40% in favor of the Training set. The split was chosen to be almost equal but at the same time due to a restricted number of data points, a slightly larger value was assigned to the Training set.




\newpage
# Data Exploration
It is a good practice to first understand the data itself before attempting to gather insights or draw conclusions [$\color{blue}{\text{(Viswanathan, 2015)}}$](#references). Exploratory data analysis is a necessary and critical process for preforming these initial investigations on data. This process allows investigators to discover patterns, spot anomalies, test hyptheses, and to scrutinize assumptions with the assistance of summary statistics and graphical representations (visualizations).


```{r, echo=FALSE}

  rRaw <- format(xRaw, big.mark="'")

```

```{r, echo=FALSE}

  cRaw <- yRaw

```


```{r, echo=FALSE}

  iFinal <- nFinal

```

```{r, echo=FALSE}

  iTesting <- nTesting

```

```{r, echo=FALSE}

  iTraining <- nTraining

```


```{r, echo=FALSE}

  old_prop <- oldest_prop

```

```{r, echo=FALSE}

  new_prop <- newest_prop

```

```{r, echo=FALSE}

  low_sale <- format(lowest_sale, big.mark="'")

```

```{r, echo=FALSE}

  high_sale <- format(highest_sale, big.mark="'")

```

```{r, echo=FALSE}

  late_sale <- latest_sale

```

```{r, echo=FALSE}

  early_sale <- earliest_sale

```

```{r, echo=FALSE}

  iNeighbors <- nNeighbors

```

```{r, echo=FALSE}

  iNum <- nNum

```

```{r, echo=FALSE}

  iChara <- nChara

```

The raw data contained `r rRaw` rows (each representing a sale of a property) and `r cRaw` columns (i.e. an Id, a Sale Price and `r cRaw -2` potential predictors of the Sale Price).

The data covered `r iNeighbors` neighborhoods in Ames, Iowa, U.S.A.

After partitioning, the Final Validation data had `r iFinal` rows, the Testing data had `r iTesting` rows and the Training data had `r iTraining` rows.

The training data covered properties built from `r old_prop` to `r new_prop` with Sale Prices ranging from as low as USD `r low_sale` to USD `r high_sale`. The training data captured actual sales from `r early_sale` to `r late_sale`.

Of the `r cRaw -2` potential predictor columns, `r iChara` were characters and the remaining `r iNum` were numeric. Later on in the analysis, some of the key character variables will be converted to numerical values to assist with the algorithm building.


```{r NumOfSalesPerNeighborhood, fig.cap="\\label{NumOfSalesPerNeighborhood}Location of Sales",     fig.width=5,  fig.height=3.333 ,   fig.align='left', echo=FALSE}

plotSalesNeighborhood %>%
  ggplot(aes(x=Neighborhood, y=count))+
  geom_bar(stat = "identity", fill="steelblue")+
  xlab("Neighborhood")+
  ylab("Number of Sales")+ 
  theme_minimal()+
  theme(panel.background = element_rect(color = "black", size = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  theme(plot.title = element_text(size=40))
```

\autoref{NumOfSalesPerNeighborhood} shows that sales occurred in many different neighborhoods and so this should be an indicator that it will be an important factor in the algorithm.


## Boruta Importance Analysis
Exploring the dataset could be difficult when the number of variables is as large as it is i.e. `r cRaw -2`. As a result, it made sense to start with a so called Boruta Feature Importance analysis to show which items are deemed important. See the [$\color{blue}{\text{appendix}}$](#boruta) for more background on how the analysis was performed.


```{r BorutaImportanceAnalysis , fig.cap="\\label{BorutaImportanceAnalysis}Boruta Importance Analysis" ,   fig.align='left', echo=FALSE}

#plot(bor.results)
plot(bor.results, cex.axis=0.5, las=2, xlab="")

```


\autoref{BorutaImportanceAnalysis} shows the relative importance of each candidate explanatory attribute.
The x-axis represents each of candidate explanatory variables.
Green boxes indicate the attributes that are relevant to prediction.
Red boxes indicate attributes that are not relevant.
Yellow boxes indicate attributes that may or may not be relevant to predicting the Sales Price.



```{r, echo=FALSE}

  iTent <- nTent

```

```{r, echo=FALSE}

  iReject <- nReject

```

```{r, echo=FALSE}

  iConfirm <- nConfirm

```

Of the `r cRaw -2` variables, `r iConfirm` were deemed important, `r iReject` were deemed not important and the remaining `r iTent` were considered "tentative".

\autoref{confirmedtable}, \autoref{tentativetable} and \autoref{rejectedtable}  list all of the variables with their "importance".    

```{r confirmedtable,    echo=FALSE}

impTable %>% 
  filter(decision == "Confirmed") %>%
  select("medianImp", "decision") %>%
  knitr::kable(caption="\\label{confirmedtable}Confirmed Variables") #'simple')

```

```{r tentativetable,    echo=FALSE}

impTable %>% 
  filter(decision == "Tentative") %>%
  select("medianImp", "decision") %>%
  knitr::kable(caption="\\label{tentativetable}Tentative Variables") #'simple')

```

```{r rejectedtable,    echo=FALSE}

impTable %>% 
  filter(decision == "Rejected") %>%
  select("medianImp", "decision") %>%
  knitr::kable(caption="\\label{rejectedtable}Rejected Variables") #'simple')

```




## Correlations    
Due to the large number of predictors, it was decided to initially focus on the exploration of the numeric variables using correlation plots. (Later in this report, the important character variables are considered.)

```{r corNumerical, fig.cap="\\label{corNumerical}Correlation Matrix for Numerical Variables",  echo=FALSE}

correlations <- cor(jTraining[, numberCols],use="everything")
corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank", tl.cex = 0.4)

```


\autoref{corNumerical} shows that of the original 35 numeric variables, 13 of them were considered important (correlation > 0.35) and this ties in with the Boruta analysis. \autoref{NamesOfInterest} lists these variables.

```{r NamesOfInterest, echo=FALSE}

NamesInterest1 %>% knitr::kable(col.name=NULL,   caption="\\label{NamesOfInterest}Important Numeric Variables") #'simple')

```


## Further Cleaning of Data and Correlations
From the Boruta analysis, of the top 20 variables only 5 were character variables. After converting to numerical values, the correlations with SalePrice are high. See \autoref{corChars}.

```{r corChars, fig.cap="\\label{corChars}Correlation Matrix for Character Variables", , echo=FALSE}

correlations <- cor(jTraining[, charColsSP],use="everything")
corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank", tl.cex = 0.4)

```

\autoref{CharactersOfInterest} lists these variables.    


```{r CharactersOfInterest, echo=FALSE}

charCols %>% knitr::kable(col.name=NULL, caption="\\label{CharactersOfInterest}Important Character Variables") #'simple')

```



## Scatter Plots
The correlation chart (\autoref{corNumerical}) indicates there is a relationship between the sale price and some numeric variables, but what the relationship is is unknown. Using simple scatter plots the relationships (or not) can be more easily seen. See \autoref{Scatter1}, \autoref{Scatter2} and \autoref{Scatter3}.  

```{r Scatter1, fig.cap="\\label{Scatter1}Sale Price vs. Year Built",  echo=FALSE}

scatterplot(SalePrice ~ YearBuilt, data = jTraining,  xlab = "Year Built", ylab = "Sale Price", grid = FALSE)

```

\autoref{Scatter1} shows that newer houses are worth more which makes sense. It is not difficult to see that the price of a property increases generally with the year built; the trend is obvious.


```{r Scatter2, fig.cap="\\label{Scatter2}Sale Price vs. Year Sold",  echo=FALSE}

scatterplot(SalePrice ~ YrSold, data = jTraining,  xlab = "Year Sold", ylab = "Sale Price", grid = FALSE)

```

\autoref{Scatter2} shows that sale prices dip in 2008 (during the financial crisis) but overall Year of Sale doesn't appear to be a major driver and can therefore be excluded.

```{r Scatter3, fig.cap="\\label{Scatter3}Sale Price vs. 1st Floor Square Footage",  echo=FALSE}

scatterplot(SalePrice ~ X1stFlrSF, data = jTraining,  xlab = "1st Floor Square Footage", ylab = "Sale Price", grid = FALSE)

```

\autoref{Scatter3} shows there were some strange outliers on the influence of first floor square footage on the sale price - it maybe bad data but they should not have a huge influence on the modelling.


## Pairs Analysis
Using a scatterplot matrix to observe the bivariate relationship between different combinations of variables is useful. Each scatter plot in the matrix visualizes the relationship between a pair of variables, allowing many relationships to be explored in one chart.

The earlier correlation plots show that the SalesPrice is well correlated with a number of the independent variables. In \autoref{Pairs3} it can be observed that some of the independent variables of interest also have high correlation with each other. An extreme case of correlated variables produces multicollinearity - a condition in which there is redundancy among the predictor variables (Perfect multicollinearity occurs when one predictor variable can be expressed as a linear combination of others.) The problem of multicollinearity is clear and should be addressed if necessary - variables should be removed until the multicollinearity is gone. 


<!-- ```{r Pairs1, echo=FALSE} -->

<!--   pairs(~YearBuilt+OverallQual+TotalBsmtSF+GrLivArea , data = jTraining , main = "Scatterplot Matrix") -->

<!-- ``` -->

<!-- ```{r Pairs2, echo=FALSE} -->

<!--   pairs(~OverallQual+TotalBsmtSF+GarageCars+GarageArea , data = jTraining, main = "Scatterplot Matrix") -->

<!-- ``` -->

```{r Pairs3, fig.cap="\\label{Pairs3}Scatterplot Matrix",  echo=FALSE}

  pairs(~GrLivArea+OverallQual+TotalBsmtSF+X1stFlrSF+GarageArea+YearBuilt , data = jTraining )

```


## Summary
Based on the above data exploration, it made sense to use a reduced list of variables going forward in the creation of the models. 
The following sets of data were used in building the models:

```{r Predictors, echo=TRUE}

numberCols

NamesInterest1

charCols

```

Along with the prediction variable itself i.e. SalePrice.

\newpage
# Analysis and Model Creation

## Average Sale Price

`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
The analysis will start with the simplest model where the predicted Sale Price is the average of all given Sale Prices.

`r text_spec("Model",  color = "#d2691e", font_size= 10)`
$$Y_{u,i}=\mu+\varepsilon_{u,i}$$

where $\varepsilon_{u,i}$ are the independent errors sampled from the same distribution.

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`
$$\hat{y}_{u,i}=\hat{\mu}$$

`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-1, echo=FALSE}
rmse_results[1:1,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## Linear Model

`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
Finally, we have our data and can build the first proper model. Since our outcome is a continuous numeric variable, we want a linear model, not a GLM.

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

The R Square is not bad, and all variables pass the Hypothesis Test. The diagonsis of residuals is also not bad. The diagnosis can be viewed below.

```{r model_lm , fig.cap="Linear Model",    echo=FALSE, message=FALSE}

layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot(model_lm)
par(mfrow=c(1,1))

```






`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-2, echo=FALSE}
rmse_results[1:2,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## LASSO Regression
For the avoidance of multicollinearity, implementing LASSO regression (see appendix [$\color{blue}{\text{(LASSO)}}$](#lasso) for background to LASSO) is not a bad idea. 


Transferring the variables into the form of matrix, we can automate the selection of variables by implementing 'lars' method in Lars package.

### Model 1 - Numeric Variables
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

```{r model_lars_1 , fig.cap="Lasso Regression - Numeric Variables",    echo=FALSE, message=FALSE}

plot(model_lars)

```

The plot is messy as the quantity of variables is intimidating. Despite that, we can still use R to find out the model with least multicollinearity. The selection 
procedure is based on the value of Marrow's cp, an important indicator of multicollinearity. The prediction can be done by the script-chosen best step and RMSE can be used
to assess the model.


`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-3, echo=FALSE}
rmse_results[1:3,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

### Model 2 - Important Variables
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

```{r model_lars_2 , fig.cap="Lasso Regression - Important Variables" , echo=FALSE, message=FALSE}

plot(model_lars_2)

```


`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-4, echo=FALSE}
rmse_results[1:4,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## Random Forest
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
Let's try training the model with an RF.   
Let's use all the variables and see how things look, since randomforest does its own feature selection.

(see appendix [$\color{blue}{\text{(Random Forest)}}$](#random_forest) for background to Random Forest).

See the appendix for some background on the Random Forest process

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

```{r plot_model_rf , fig.cap="\\label{plot_model_rf}Random Forest model",    echo=FALSE, message=FALSE}

plot(model_rf)

```


`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-5, echo=FALSE}
rmse_results[1:5,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## XGB - eXtreme Graded Boost
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     

See the appendix for some background on the XGB process

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-6, echo=FALSE}
rmse_results[1:6,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## Final Validation
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
As can be seen the XGB method gave the lowest RMSE therefore this is the model that was chosen.

The model was then tested with the final validation data set to see if the RMSE was in line with expectations.

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-7, echo=FALSE}
rmse_results[1:7,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

\newpage
# Results    

Definitely need to write some text here about the results.    


```{r RMSE-summary-8, echo=FALSE}
rmse_results %>% 
  kbl(caption = "Overall Results" ) %>%
  kable_styling(full_width = FALSE, position="left")
```





\newpage
# Conclusion
Through the gradual increase in complexity of the model, the Regularized Movie and User Effect model produced an improvement of nearly 19% over the original model.

I believe with more work using the regularized techniques being applied to the Time and Genre effects, the overall model could be further improved.

Running the analysis on a relatively old PC also caused a few issues and for the further work (suggested below) it would warrant upgrading to a more powerful machine.

I would like to take the opportunity to thank the staff and other students at HarvardX for producing a very enjoyable, stimulating and often challenging course. In particular I would like to thank Prof. Irizarry for his knowledge and clarity with the online videos and assessment material.

Whilst I was aware of the topics of r, probability and regression from my former studies it has been many years since I have used them and the topics of wrangling and machine learning were completely new to me. They were topics that I thoroughly enjoyed learning albeit at a level that I know is just scratching the surface. I believe my data science journey has just begun...

(Un)fortunately due to my current job situation, I was able to complete the course in a shorter than recommended period. I believe this was ultimately very beneficial to me and allowed me to keep a high level of focus. I'm not sure I would have managed to study so hard and maintain a full time job at the same time had the situation occurred!!


## Limitations
Small data set

## Future Work
Whilst the above analysis has already produced a lot of insights and a meaningful model, further investigations could be carried out along the following lines:   

* Find a better way to pick different variables - correlations
* Extend the data to other areas of the country
* Increase data size
* Speak with real estate agents to see if there is other data that could/should be incorporated/ignored
* Real Estate special sauce
  + Add something here....   
  + And here...
    - What's happening here??
    - And here ???
* Go back further in time

* The following points relate to the actual analysis that is performed:   
  + Factor analysis   
    - Repeat analysis with a larger dataset - PC already blows up with a relatively small data set   
    - Use the recommenderlab package to extend the type of analysis performed   
  + Explore other (currently) unknown models/techniques 
  + Ensemble model
  + PCA
  + Neural network - look at the tutorial




\newpage
\appendix
# Appendix   
The following sections provide some background to the analysis that was carried out for this report.   

## Boruta Importance Analysis{#boruta}
Earlier in this report, the R package Boruta was used to select all relevant variables from the House Price data set  [$\color{blue}{\text{(Kursa and Rudnicki, 2020)}}$](#references). This section gives a little background to the algorithm. 

The original article describes the R package Boruta, implementing a novel feature selection algorithm for finding all relevant variables. The algorithm is designed as a wrapper around a Random Forest classification algorithm. It iteratively removes the features which are proved by a statistical test to be less relevant than random probes. The Boruta package provides a convenient interface to the algorithm. 

Feature selection is often an important step in applications of machine learning methods and there are good reasons for this. Modern data sets are often described with far too many variables for practical model building. Usually most of these variables are irrelevant to the classification, and obviously their relevance is not known in advance. There are several disadvantages of dealing with overlarge feature sets. One is purely technical - dealing with
large feature sets slows down algorithms, takes too many resources and is simply inconvenient. Another is even more important - many machine learning algorithms exhibit a decrease of accuracy when the number of variables is significantly higher than optimal (Kohavi and John 1997). Therefore selection of the small (possibly minimal) feature set giving best possible classification results is desirable for practical reasons. This problem, known as minimal - optimal problem (There is a 2007 reference here) has been intensively studied and there are plenty of algorithms which were developed to reduce feature set to a manageable size.

Nevertheless, this very practical goal shadows another very interesting problem - the identification of all attributes which are in some circumstances relevant for classification, the so-called all-relevant problem. Finding all relevant attributes, instead of only the non-redundant ones, may be very useful in itself. In particular, this is necessary when one is interested in understanding mechanisms related to the subject of interest, instead of merely building a black box predictive model. For example, when dealing with results of gene expression measurements in context of cancer, identification of all genes which are related to cancer is necessary for complete understanding of the process, whereas a minimal-optimal set of genes might be more useful as genetic markers. A good discussion outlining why finding all relevant attributes is important is given by Nilsson et al. (2007).

The all-relevant problem of feature selection is more difficult than usual minimal-optimal one. One reason is that we cannot rely on the classification accuracy as the criterion for selecting the feature as important (or rejecting it as unimportant). The degradation of the classification accuracy, upon removal of the feature from the feature set, is sufficient to declare the feature important, but lack of this effect is not sufficient to declare it unimportant. One therefore needs another criterion for declaring variables important or unimportant. Moreover, one cannot use filtering methods, because the lack of direct correlation between a given feature and the decision is not a proof that this feature is not important in conjunction with the other
features (Guyon and Elisseeff 2003). One is therefore restricted to wrapper algorithms, which are computationally more demanding than filters.

In a wrapper method the classifier is used as a black box returning a feature ranking, therefore one can use any classifier which can provide the ranking of features. For practical reasons, a classifier used in this problem should be both computationally efficient and simple, possibly without user defined parameters.

The current paper presents an implementation of the algorithm for finding all relevant features in the information system in a R (R Development Core Team 2010) package Boruta (available from the Comprehensive R Archive Network at http://CRAN.R-project.org/package=Boruta). The algorithm uses a wrapper approach built around a random forest (Breiman 2001) classifier (Boruta is a god of the forest in the Slavic mythology). The algorithm is an extension of the idea introduced by Stoppiglia, Dreyfus, Dubois, and Oussar (2003) to determine relevance by comparing the relevance of the real features to that of the random probes. Originally this idea was proposed in the context of filtering, whereas here it is used in the wrapper algorithm. In the remaining sections of this article firstly a short description of the algorithm is given, followed by the examples of its application on a real-world and artificial data set.

The following is a high-level outline of the algorithm:     

1. A copy is made of each explanatory variable. The values in these copies are permuted to remove any correlation with the target variable. The copies are called Shadow variables.   

2. A random forest model is fitted on this expanded data set.     

3. For each variable (the original and Shadow) the Z-score of the loss in accuracy is calculated. The Z-score is the average loss divided by the standard deviation.

4. Keep track of the original attributes z-score that score higher than the maximum of z-score of the shadow attributes.     

5. Repeat the above steps numerous times. Original attributes that are significantly--statistically--higher then the maximum z-score of shadow attributes are deemed relevant to the prediction. Attributes that are significantly below the maximum z-score of shadow attributes are deemed not relevant.


## LASSO Regression{#lasso}
In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model. It was originally introduced in geophysics and later by Robert Tibshirani who coined the term. [$\color{blue}{\text{(Wikipedia)}}$](#references)

Lasso was originally formulated for linear regression models. This simple case reveals a substantial amount about the estimator. These include its relationship to ridge regression and best subset selection and the connections between lasso coefficient estimates and so-called soft thresholding. It also reveals that (like standard linear regression) the coefficient estimates do not need to be unique if covariates are collinear.

Though originally defined for linear regression, lasso regularization is easily extended to other statistical models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators. Lasso's ability to perform subset selection relies on the form of the constraint and has a variety of interpretations including in terms of geometry, Bayesian statistics and convex analysis.

*From Statistics How To* [$\color{blue}{\text{(From Statistics How To)}}$](#references)

Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.

The acronym “LASSO” stands for Least Absolute Shrinkage and Selection Operator.

Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and eliminated from the model. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models. On the other hand, L2 regularization (e.g. Ridge regression) doesn’t result in elimination of coefficients or sparse models. This makes the Lasso far easier to interpret than the Ridge.

Lasso solutions are quadratic programming problems. The goal of the algorithm is to minimize:

$$\sum_{i=i}^{N} (y_{i} - \sum_{j}x_{i,j}\beta_{j})^2+\lambda\sum_{j=1}^{p}\mid\beta_{j}\mid$$

Which is the same as minimizing the sum of squares with constraint $\sum\mid\beta_{j}\mid \le s$  . Some of the $\beta$s are shrunk to exactly zero, resulting in a regression model that is easier to interpret.

A tuning parameter, $\lambda$, controls the strength of the L1 penalty. $\lambda$ is basically the amount of shrinkage:

* When $\lambda$ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.
* As $\lambda$ increases, more and more coefficients are set to zero and eliminated (theoretically, when $\lambda$ = $\infty$, all coefficients are eliminated).
* As $\lambda$ increases, bias increases.
* As $\lambda$ decreases, variance increases.
* If an intercept is included in the model, it is usually left unchanged.

## Random Forest{#random_forest}
Random forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.

Random decision forests correct for decision trees' habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees (see next section). However, data characteristics can affect their performance [$\color{blue}{\text{(Wikipedia)}}$](#references).

Random forests are frequently used as "blackbox" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.

The first step is bootstrap aggregation or bagging. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, bootstrapping is used to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees randomly different, and the combination of trees is the forest. The specific steps are as follows.

1. Build $B$ decision trees using the training set. We refer to the fitted models as $T_{1}, T_{2}, \dots , T_{B}$. We later explain how we ensure they are different.

2. For every observation in the test set, form a prediction $\hat{y}_{j}$ using tree $T_{j}$.

3. For continuous outcomes, form a final prediction with the average $\hat{y} = \frac{1}{B} \sum_{j=1}^{B} \hat{y}_{j}$. For categorical data classification, predict $\hat{y}$ with majority vote (most frequent class among $\hat{y}_{1}, \dots , \hat{y}_{T}$).

So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let $N$ be the number of observations in the training set. To create $T_{j}$, $j=1, \dots , B$ from the training set we do the following:

1. Create a bootstrap training set by sampling $N$ observations from the training set with replacement. This is the first way to induce randomness.

2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.

As can be observed in \autoref{plot_model_rf} as more trees are added to the model, the error rate changes, the accuracy improves and with about 80 trees the accuracy stabilizes.

Random forest performs better in all the examples we have considered. However, a disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine variable importance. To define variable importance we count how often a predictor is used in the individual trees. You can learn more about variable importance in an advanced machine learning book109. The caret package includes the function varImp that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section.






## XGBoost (e$X$treme $G$radient $B$oosting){#xgb}
### Background     
Ensemble models have become a standard tool for predictive modeling [$\color{blue}{\text{(Bruce, Bruce and Gedeck, 2020)}}$](#references). Boosting is a general technique to create an ensemble of models. It was developed around the same time as bagging. Like bagging, boosting is most commonly used with decision trees. Despite their similarities, boosting takes a very different approach - one that comes with more bells and whistles. As a result, while bagging can be done with relatively little tuning, boosting requires much greater care in its application. If these two methods were cars, bagging could be considered a Honda Accord (reliable and steady), whereas boosting could be considered a Porsche (powerful but requires more care).

In linear regression models, the residuals are often examined to see if the fit can be improved. Boosting takes this concept much further and fits a series of models, in which each successive model seeks to minimize the error of the previous model. Several variants of the algorithm are commonly use: Adaboost, gradient boosting and stochastic gradient boosting.

The most widely used public domain software for boosting is XGBoost (and the one used in this report). It is an implementation of stochastic gradient boosting that is both computationally efficient and made up of many options/paramters. Two very important parameters are subsample, which controls the fraction of observations that should be sampled at each iteration, and eta, a shrinkage factor applied to $\alpha_{m}$ in the boosting algorithm.

Using subsample makes boosting act like the random forest except that the sampling is done without replacement. The shrinkage parameter eta is helpful to prevent overfitting by reducing the change in the weights (a smaller change in the weights means the algorithm is less likely to overfit to the training set).

### Summary
* Boosting is a class of ensemble models based on fitting a sequence of models, with more weight given to records with large errors in successive rounds.
* Stochastic gradient boosting is the most general type of boosting and offers the best performance. The most common form of stochastic gradient boosting uses tree models.
* XGBoost is a popular and computationally efficient software package for stochastic gradient boosting; it is available in all common languages used in data science.
* Boosting is prone to overfitting the data, and the hyperparameters need to be tuned to avoid this.
* Regularization is one way to avoid overfitting by including a penalty term on the number of parameters ( e.g., tree size) in a model.
* Cross-validation (see next section) is especially important for boosting due to the large number of hyperparameters that need to be set.

### Tutorial
The XGBoost documentation site [$\color{blue}{\text{(XGBoost Documentation)}}$](#references) provides further insights and tutorials on this very powerful tool.





## Cross Validation
### Background     
A common goal of machine learning is to find an algorithm that produces predictors $\hat{Y}$ of an outcome $Y$ that minimizes the MSE:
  
$$MSE = E \left\{ \frac{1}{N} \sum_{i=i}^{N} (\hat{Y}_i - Y_i)^2 \right\}$$

When there is only one dataset, it is possible to estimate the MSE with the observed MSE like this:

$$\hat{MSE} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2$$

These two are referred to as the true error and apparent error, respectively.

There are two important characteristics of the apparent error that should always be kept in mind:

* Because the data is random, the apparent error is a random variable. For example, the given dataset may be a random sample from a larger population. An algorithm may have a lower apparent error than another algorithm due to luck.

* If an algorithm is trained on the same dataset that is used to compute the apparent error, it might result in overtraining. In general, when this action is performed, the apparent error will be an underestimate of the true error.

Cross-validation is a technique that permits the alleviation of both these problems. To understand cross-validation, it helps to think of the true error, a theoretical quantity, as the average of many apparent errors obtained by applying the algorithm to $B$ new random samples of the data, none of them used to train the algorithm. The true error can be thought of as:

$$\frac{1}{B} \sum_{b=1}^{B} \frac{1}{N}  \sum_{i=i}^{N} (\hat{y}_{i}^{b} - y_{i}^{b})^2  $$

with $B$ a large number that can be thought of as practically infinite. As already mentioned, this is a theoretical quantity because there is only one available set of outcomes: $y_1, \dots ,y_n$ . Cross validation is based on the idea of imitating the theoretical setup above as best as possible with the available data. To do this, it is necessary to generate a series of different random samples. There are several approaches that can be employed, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error.

For most machine learning algorithms it is necessary to select parameters, let's say $\lambda$, that minimises the value of MSE. It is known that if optimsation and evaluation take place on the training set, overtraining will occur. It is also the golden rule of machine learning to not use the original validation data set and this is where cross-validation is most useful.

### K-fold cross-validation    
One of the possibilities (and the one used in the regularization part of this project) is K-fold cross-validation. This technique divides the data into $K$ subsets (folds) of almost equal size. Out of these $K$ folds, one subset is used as a validation set and the others are involved in training the model.

The following describes the complete working procedure of this method:

1. Split the training dataset randomly into K subsets

2. Use K-1 subsets for the training of the model

3. Test the model against that one subset which was left out in the previous step: 

$$\hat{MSE_{b}}(\lambda) = \frac{1}{M}  \sum_{i=1}^{M} (\hat{y}_{i}^{b}(\lambda) - y_{i}^{b})^2 $$

4. Repeat the above steps $K$ times i.e. until the model is trained and tested on all subsets. This generates: 

$$\hat{MSE_{1}}(\lambda) , \hat{MSE_{2}}(\lambda) , \dots , \hat{MSE_{K}}(\lambda)$$

5. Generate an overall prediction error by taking the average of prediction errors in every case:

$$\hat{MSE}(\lambda) = \frac{1}{B} \sum_{b=1}^{K} \hat{MSE_{b}}(\lambda)$$

6. The above method describes one iteration of the process with one value of the tuning parameter $\lambda$ being fixed. In general, the above is repeated with several different values of the tuning parameter to find the optimal value of $\lambda$ that minimises the MSE.

7. Fit the final model with the obtained optimal tuning parameter(s).

8. Finally test the results from the previous step on the original independent validation dataset using the final model. 

In this project, it was decided to use a value of 5 for $K$. Large values of $K$ are preferable because the training data better imitates the original dataset. However, larger values of $K$ will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of $K=5$ and $K=10$ are popular.

### Other techniques and implementations     
One way to improve the variance of the final estimate is to take more samples. To do this, it would no longer require the training set to be partitioned into non-overlapping sets. Instead, just pick $K$ sets of some size at random.

One version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach has some advantages and is generally referred to as the bootstrap.   

XGBoost, used in this report, uses cross validation in its implementation.

## Data Source
The Ames Housing dataset, used in this report, was compiled by Dean De Cock for use in data science education. It's an alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. 

The original dataset can be found under : [$\color{blue}{\text{https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data}}$](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).

For the purposes of this report, a copy has been stored under the "data" directory of the project's github repository :  [$\color{blue}{\text{https://github.com/ostlerjo/CapstoneHousePrices.git}}$](https://github.com/ostlerjo/CapstoneHousePrices.git)

To be precise located here : [$\color{blue}{\text{https://github.com/ostlerjo/CapstoneHousePrices/tree/master/data}}$](https://github.com/ostlerjo/CapstoneHousePrices/tree/master/data)

## References{#references}
Galwey, N. (2014). Introduction to mixed modelling: Beyond regression and analysis of variance.  

Irizarry, R. A. (2020). Introduction to data science: Data analysis and prediction algorithms with R : [$\color{blue}{\text{https://doi.org/10.1201/9780429341830}}$](https://doi.org/10.1201/9780429341830).

Kaggle : Your home for data science :  [ $\color{blue}{\text{https://www.kaggle.com }}$ ](https://www.kaggle.com )  

Viswanathan, V. (2015). R data analysis cookbook: Over 80 recipes to help you breeze through your data analysis projects using R. 

Kursa, M. and Rudnicki, W. (2020). Journal of Statistical Software - Feature Selection with the Boruta Package :  [$\color{blue}{\text{https://www.jstatsoft.org/index.php/jss/article/view/v036i11/v36i11.pdf}}$](https://www.jstatsoft.org/index.php/jss/article/view/v036i11/v36i11.pdf)

LASSO statistics from Wikipedia : [ $\color{blue}{\text{https://en.wikipedia.org/wiki/Lasso }}$ ](https://en.wikipedia.org/wiki/Lasso_(statistics) )    

Statistics How To : [$\color{blue}{\text{https://www.statisticshowto.com/lasso-regression/}}$](https://www.statisticshowto.com/lasso-regression/)

Random Forest from Wikipedia :  [ $\color{blue}{\text{https://en.wikipedia.org/wiki/Random-forest }}$ ](https://en.wikipedia.org/wiki/Random_forest )
        
Peter Bruce, Andrew Bruce and Peter Gedeck (2020). Practical Statistics for Data Scientists.

XGBoost Documentation : [$\color{blue}{\text{https://xgboost.readthedocs.io/en/latest/index.html}}$](https://xgboost.readthedocs.io/en/latest/index.html)

XGBoost R Tutorial : [$\color{blue}{\text{https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html}}$](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html) 

## Course Material
Links to edX and HarvardX :   
[ $\color{blue}{\text{https://www.edx.org }}$ ](https://www.edx.org )     
[ $\color{blue}{\text{https://www.edx.org/school/harvardx }}$ ](https://www.edx.org/school/harvardx )     

Link to this course (including some of the theory quoted above):   
[ $\color{blue}{\text{https://www.edx.org/professional-certificate/harvardx-data-science }}$ ](https://www.edx.org/professional-certificate/harvardx-data-science ) 

