---
title: "Capstone House Prices"
subtitle: 'HarvardX (edX) Data Science Professional Certificate: PH125.9x Capstone'
author: "Jonathan Ostler"
date: "`r format(Sys.Date())`"
output: 
  pdf_document: 
    df_print: kable
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: yes
  html_document: default
include-before: '`\newpage{}`{=latex}'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-global-env , eval=TRUE, echo=FALSE}

#Ensures all the data created from the r script is available to the Rmd file
#Process - 
  # 1 - Run the script
  # 2 - Save the .Rmd file
  # 3 - Click on the Knit button/icon at the top of the IDE
  # 4 - This is the "pull" method
  # 5 - Alternatively set eval = FALSE for this chunk and "push" from console ====> #  rmarkdown::render("CapstoneHousePrices.Rmd")

load(file = "CapstoneHousePrices.RData")

```

\newpage
# Abstract
Data science has become the super-power behind the success of many businesses of today.  These companies need and use data science approaches like machine learning to extract patterns, trends, and other actionable information from large data sets (Galwey, 2014).  In turn, companies use these findings to improve performance, grow, and  perhaps most importantly, to provide better products and services to consumers (Irizarry, 2020).

Not surprisingly, recommendation and prediction systems are increasingly used by businesses to provide personalized insights to consumers. Somewhat famously, in 2009 Netflix awarded a $1,000,000 prize to the data science team who improved the streaming giantâ€™s movie recommendation algorithm by 10%.  The Netflix competition was the inspiration for this project and subject of this report. 

The first goal of this project was to create a movie recommendation algorithm and model from a MovieLens dataset from GroupLens utilizing R Markdown in RStudio.  The selected dataset comprises 10 million unique movie ratings (observation).  Code provided within the course parses the dataset into a training set (edx) and a reserved dataset (validation) held for final testing and validation of the resulting algorithm (Galwey, 2014). The second goal, and perhaps most important goal, was to create and train a final algorithm (model) that predicts movie ratings with a RMSE (root mean square error) of less than 0.86490 (Project Target) when compared to the actual ratings of the validation dataset.




One of the most popular competitions on kaggle is the House Prices: Advanced Regression Techniques. The original data comes from the publication Dean De Cock "Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project", Journal of Statistics Education, Volume 19, Number 3 (2011). Recently a 'demonstration' notebook has been published "First place is meaningless in this way!" that extracts the 'solution' from the full dataset. Now that the 'solution' is readily available the possibility has opened for people to reproduce the competition at home without any daily submission limit. This will open up the possibility of experimenting with advanced techniques such as pipelines with/or various estimators/models in the same notebook, extensive hyper-parameter tuning etc. And all without the risk of 'upsetting' the public leaderboard. Simply download this solution.csv file and import it into your script or notebook and evaluate the Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the data in this file.    


Data was based on the sales of properties in the XXX region of YYYY during the early 2000's etc etc. Trying to predict the Sales Price given the array of predictors such as year built, number of rooms etc. etc.


This report details some of the techniques used in the winning solution and how it was built up from first principles.

The Netflix challenge used the standard error loss function, Root Mean Squared Errors (**RMSE**): 

$$RMSE = \sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i}-y_{u,i})^2}$$
with the winner being chosen based on the lowest RMSE value.

This report shows the evolution of the RMSE value from an initial simple solution to the ultimate solution as more complex models are created.   

The GitHub repository for this report can be found at
https://github.com/ostlerjo/CapstoneHousePrices.git


\newpage
# Introduction

## Libraries used

```{r installing-libs, eval=TRUE, echo=FALSE, message=FALSE}

if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(Boruta)) install.packages("Boruta", repos = "http://cran.us.r-project.org")
if(!require(plyr)) install.packages("plyr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(FeatureHashing)) install.packages("FeatureHashing", repos = "http://cran.us.r-project.org")
if(!require(Matrix)) install.packages("Matrix", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(dummies)) install.packages("dummies", repos = "http://cran.us.r-project.org")
if(!require(Metrics)) install.packages("Metrics", repos = "http://cran.us.r-project.org")
if(!require(kernlab)) install.packages("kernlab", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(car)) install.packages("car", repos = "http://cran.us.r-project.org")
if(!require(lars)) install.packages("lars", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")

```



The following r libraries were used to generate the data, perform the analysis and create the algorithms detailed in this report:

```{r loading-libs, eval=TRUE, echo=TRUE,  message=FALSE}

library(caret)
library(data.table)
library(Boruta)
library(plyr)
library(dplyr)
library(pROC)
library(tidyverse)
library(FeatureHashing)
library(Matrix)
library(xgboost)
require(randomForest)
require(ggplot2)
library(stringr)
library(dummies)
library(Metrics)
library(kernlab)
library(corrplot)
library(car)
library(lars)
library(kableExtra)

```

## Data used
Data was obtained from kaggle.com. This is a great source of data sets. The original data set came from XYZ.


\newpage
# Data Cleaning and Preparation

## Initial Data Cleaning

Due to the way r treats variable names, three columns were mutated to put a dummy X character in front e.g. 1stFlrSF became X1stFlrSF etc.

Secondly several columns had false NAs in them that would cause the code to give errors e.g. the column "Alley" had a set of NAs which in fact meant "No Alley". As a consequence, all of these types of values were explored and then updated to their correct values.

Finally a couple of numeric columns also had NAs when they should in fact mean 0. As a consequence, all of these types of values were updated to be 0 rather than NA.

## Data Partitioning
Two sets of data were available i.e. a train set and a test set. Since the data was originally going to be used in a competition to predict Sale Prices, the test set provided did not have a Sale Price column. As a consequence for this project where predictive results needed to be reported, the train data set was used for training, testing and the final validation through partitioning.

Initially the train set was partitioned into a Modeling set and a Final Validation set. The split was 90% / 10% in favor of the Modeling set. The small percentage assigned to the Final Validation set was because the overall data set did not have so many values (around 1'500) and most of the data should be used for modeling. The Final Validation set was set aside and not used again until the Final RMSE calculation. 

Next the Modeling set was further partitioned into a Training set and a Testing set that would be used for analysis, model creation, training and initial validation. The split was 60 % / 40% in favor of the Training set. The split was chosen to be almost equal but at the same time due to a restricted number of data points, a slightly larger value was assigned to the Training set.

\newpage
# Data Exploration
It is a good practice to first understand the data itself before attempting to gather insights or draw conclusions (Viswanathan, 2015). Exploratory data analysis is a necessary and critical process for preforming these initial investigations on data. This process allows investigators to discover patterns, spot anomalies, test hyptheses, and to scrutinize assumptions with the assistance of summary statistics and graphical representations (visualizations).


```{r, echo=FALSE}

  rRaw <- format(xRaw, big.mark="'")

```

```{r, echo=FALSE}

  cRaw <- yRaw

```


```{r, echo=FALSE}

  iFinal <- nFinal

```

```{r, echo=FALSE}

  iTesting <- nTesting

```

```{r, echo=FALSE}

  iTraining <- nTraining

```


```{r, echo=FALSE}

  old_prop <- oldest_prop

```

```{r, echo=FALSE}

  new_prop <- newest_prop

```

```{r, echo=FALSE}

  low_sale <- format(lowest_sale, big.mark="'")

```

```{r, echo=FALSE}

  high_sale <- format(highest_sale, big.mark="'")

```

```{r, echo=FALSE}

  late_sale <- latest_sale

```

```{r, echo=FALSE}

  early_sale <- earliest_sale

```

```{r, echo=FALSE}

  iNeighbors <- nNeighbors

```

```{r, echo=FALSE}

  iNum <- nNum

```

```{r, echo=FALSE}

  iChara <- nChara

```

The raw data contained `r rRaw` rows (each representing a sale of a property) and `r cRaw` columns (i.e. an Id, a Sale Price and `r cRaw -2` potential predictors of the Sale Price).

The data covered `r iNeighbors` neighborhoods in Ames????

After partitioning, the Final Validation data had `r iFinal` rows, the Testing data had `r iTesting` rows and the Training data had `r iTraining` rows.

The training data covered properties built from `r old_prop` to `r new_prop` with Sale Prices ranging from as low as USD `r low_sale` to USD `r high_sale`. The training data captured actual sales from `r early_sale` to `r late_sale`.

Of the `r cRaw -2` potential predictor columns, `r iChara` were characters and the remaining `r iNum` were numeric. Later on in the analysis, some of the key character variables will be converted to numerical values to assist with the algorithm building.


```{r NumOfSalesPerNeighborhood, fig.cap="\\label{NumOfSalesPerNeighborhood}Location of Sales",     fig.width=5,  fig.height=3.333 ,   fig.align='left', echo=FALSE}

plotSalesNeighborhood %>%
  ggplot(aes(x=Neighborhood, y=count))+
  geom_bar(stat = "identity", fill="steelblue")+
  xlab("Neighborhood")+
  ylab("Number of Sales")+ 
  theme_minimal()+
  theme(panel.background = element_rect(color = "black", size = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  theme(plot.title = element_text(size=40))
```

\autoref{NumOfSalesPerNeighborhood} shows that sales occurred in many different neighborhoods and so this should be an indicator that it will be an important factor in the algorithm.


## Boruta Importance Analysis
Exploring the dataset could be difficult when the number of variables is as large as it is i.e. `r cRaw -2`. As a result, it makes sense to start with a so called Boruta Feature Importance analysis to show which items are deemed important. See appendix for details of the analysis.

```{r BorutaImportanceAnalysis , fig.cap="\\label{BorutaImportanceAnalysis}Boruta Importance Analysis" ,   fig.align='left', echo=FALSE}

plot(bor.results)

```


\autoref{BorutaImportanceAnalysis} shows the relative importance of each candidate explanatory attribute.
The x-axis represents each of candidate explanatory variables.
Green color indicates the attributes that are relevant to prediction.
Red indicates attributes that are not relevant.
Yellow color indicates attributes that may or may not be relevant to predicting the response variable.



```{r, echo=FALSE}

  iTent <- nTent

```

```{r, echo=FALSE}

  iReject <- nReject

```

```{r, echo=FALSE}

  iConfirm <- nConfirm

```

Of the `r cRaw -2` variables, `r iConfirm` were deemed important, `r iReject` were deemed not important and the remaining `r iTent` were considered "tentative".

\autoref{confirmedtable}, \autoref{tentativetable} and \autoref{rejectedtable}  list all of the variables with their "importance".    

```{r confirmedtable,    echo=FALSE}

impTable %>% 
  filter(decision == "Confirmed") %>%
  select("medianImp", "decision") %>%
  knitr::kable(caption="\\label{confirmedtable}Confirmed Variables") #'simple')

```

```{r tentativetable,    echo=FALSE}

impTable %>% 
  filter(decision == "Tentative") %>%
  select("medianImp", "decision") %>%
  knitr::kable(caption="\\label{tentativetable}Tentative Variables") #'simple')

```

```{r rejectedtable,    echo=FALSE}

impTable %>% 
  filter(decision == "Rejected") %>%
  select("medianImp", "decision") %>%
  knitr::kable(caption="\\label{rejectedtable}Rejected Variables") #'simple')

```




## Correlations    
Lot of dependents therefore mainly focused on the exploration of numeric variables to begin with using correlation plots.
The numeric variables are sorted out before turning some of the important character variables into numeric form.

All original numeric values

```{r corNumerical, fig.cap="\\label{corNumerical}Correlation Matrix for Numerical Variables",  echo=FALSE}

correlations <- cor(jTraining[, numberCols],use="everything")
corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank")

```


\autoref{corNumerical} shows that 13 of the numeric variables were considered important (correlation > 0.35) and this ties in with the Boruta analysis. \autoref{NamesOfInterest} lists these variables.

```{r NamesOfInterest, echo=FALSE}

NamesInterest1 %>% knitr::kable(caption="\\label{NamesOfInterest}Important Numeric Variables") #'simple')


```


## Further cleaning of data and correlations
From the Boruta analysis, of the top 20 variables only 5 are character variables. After converting to numerical values, the correlations with salePrice are high. See \autoref{corChars}.

```{r corChars, fig.cap="\\label{corChars}Correlation Matrix for Character Variables", , echo=FALSE}

correlations <- cor(jTraining[, charColsSP],use="everything")
corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank")

```

\autoref{CharactersOfInterest} lists these variables.    


```{r CharactersOfInterest, echo=FALSE}

charCols %>% knitr::kable(caption="\\label{CharactersOfInterest}Important Character Variables") #'simple')


```



## Scatter plots
The correlation chart (\autoref{corNumerical}) indicates there is a relationship between the sale price and some numeric variables, but what the relationship is is unknown. Using simple scatter plots the relationships (or not) can be more easily seen. See \autoref{Scatter1}, \autoref{Scatter2} and \autoref{Scatter3}.  

```{r Scatter1, fig.cap="\\label{Scatter1}Sale Price vs. Year Built",  echo=FALSE}

scatterplot(SalePrice ~ YearBuilt, data = jTraining,  xlab = "Year Built", ylab = "Sale Price", grid = FALSE)

```

\autoref{Scatter1} shows that newer houses are worth more which makes sense. It is not difficult to see that the price of a property increases generally with the year built, the trend is obvious.


```{r Scatter2, fig.cap="\\label{Scatter2}Sale Price vs. Year Sold",  echo=FALSE}

scatterplot(SalePrice ~ YrSold, data = jTraining,  xlab = "Year Sold", ylab = "Sale Price", grid = FALSE)

```

\autoref{Scatter2} shows that sale prices dip in 2008 (during the financial crisis) but overall Year of Sale doesn't appear to be a major driver.

```{r Scatter3, fig.cap="\\label{Scatter3}Sale Price vs. 1st Floor Square Footage",  echo=FALSE}

scatterplot(SalePrice ~ X1stFlrSF, data = jTraining,  xlab = "1st Floor Square Footage", ylab = "Sale Price", grid = FALSE)

```


\autoref{Scatter3} shows there were some strange outliers on first floor square footage - it maybe bad data but they should not have a huge influence.


## Pairs analysis
Using a scatterplot matrix to observe the bivariate relationship between different combinations of variables is useful. Each scatter plot in the matrix visualizes the relationship between a pair of variables, allowing many relationships to be explored in one chart.

The earlier correlation plots show that the SalesPrice is well correlated with a number of the independent variables. From \autoref{Pairs3} it can be observed that some of the independent variables of interest also have high correlation with each other. An extreme case of correlated variables produces multicollinearity - a condition in which there is redundancy among the predictor variables (Perfect multicollinearity occurs when one predictor variable can be expressed as a linear combination of others.) The problem of multicollinearity is clear and should be addressed if necessary - variables should be removed until the multicollinearity is gone. 


<!-- ```{r Pairs1, echo=FALSE} -->

<!--   pairs(~YearBuilt+OverallQual+TotalBsmtSF+GrLivArea , data = jTraining , main = "Scatterplot Matrix") -->

<!-- ``` -->

<!-- ```{r Pairs2, echo=FALSE} -->

<!--   pairs(~OverallQual+TotalBsmtSF+GarageCars+GarageArea , data = jTraining, main = "Scatterplot Matrix") -->

<!-- ``` -->

```{r Pairs3, fig.cap="\\label{Pairs3}Scatterplot Matrix",  echo=FALSE}

  pairs(~GrLivArea+OverallQual+TotalBsmtSF+X1stFlrSF+GarageArea+YearBuilt , data = jTraining )

```


## Summary
Based on the above data exploration, it makes sense to use a reduced list of variables going forward in the creation of the models. 
The following sets of data will be used in building the models:

```{r Predictors, echo=TRUE}

numberCols

NamesInterest1

charCols

```

And then the prediction variable itself is SalePrice.

\newpage
# Analysis and Model Creation

## Average Sale Price

`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
The analysis will start with the simplest model where the predicted Sale Price is the average of all given Sale Prices.

`r text_spec("Model",  color = "#d2691e", font_size= 10)`
$$Y_{u,i}=\mu+\varepsilon_{u,i}$$

where $\varepsilon_{u,i}$ are the independent errors sampled from the same distribution.

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`
$$\hat{y}_{u,i}=\hat{\mu}$$

`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-1, echo=FALSE}
rmse_results[1:1,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## Linear Model

`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
Finally, we have our data and can build the first proper model. Since our outcome is a continuous numeric variable, we want a linear model, not a GLM.

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

The R Square is not bad, and all variables pass the Hypothesis Test. The diagonsis of residuals is also not bad. The diagnosis can be viewed below.

```{r model_lm , fig.cap="Linear Model",    echo=FALSE, message=FALSE}

layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot(model_lm)
par(mfrow=c(1,1))

```






`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-2, echo=FALSE}
rmse_results[1:2,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## LASSO regression
For the avoidance of multicollinearity, implementing LASSO regression (see appendix for background to LASSO) is not a bad idea. 

Transferring the variables into the form of matrix, we can automate the selection of variables by implementing 'lars' method in Lars package.

### Model 1 - Numeric Variables
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

```{r model_lars_1 , fig.cap="Lasso Regression - Numeric Variables",    echo=FALSE, message=FALSE}

plot(model_lars)

```

The plot is messy as the quantity of variables is intimidating. Despite that, we can still use R to find out the model with least multicollinearity. The selection 
procedure is based on the value of Marrow's cp, an important indicator of multicollinearity. The prediction can be done by the script-chosen best step and RMSE can be used
to assess the model.


`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-3, echo=FALSE}
rmse_results[1:3,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

### Model 2 - Important Variables
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

```{r model_lars_2 , fig.cap="Lasso Regression - Important Variables" , echo=FALSE, message=FALSE}

plot(model_lars_2)

```


`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-4, echo=FALSE}
rmse_results[1:4,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## Random Forest
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
Let's try training the model with an RF.   
Let's use all the variables and see how things look, since randomforest does its own feature selection.

See the appendix for some background on the Random Forest process

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

```{r model_rf , fig.cap="Random Forest model",    echo=FALSE, message=FALSE}

plot(model_rf)

```


`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-5, echo=FALSE}
rmse_results[1:5,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## XGB - eXtreme Graded Boost
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     

See the appendix for some background on the XGB process

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-6, echo=FALSE}
rmse_results[1:6,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

## Final Validation
`r text_spec("Motivation",  color = "#d2691e", font_size= 10)`     
As can be seen the XGB method gave the lowest RMSE therefore this is the model that was chosen.

The model was then tested with the final validation data set to see if the RMSE was in line with expectations.

`r text_spec("Model",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("Prediction",  color = "#d2691e", font_size= 10)`     
XXX

`r text_spec("RMSE Results",  color = "#d2691e", font_size= 10)`
```{r RMSE-summary-7, echo=FALSE}
rmse_results[1:7,] %>% 
  kbl() %>%
  kable_styling(full_width = FALSE, position="left")
```

###### $\color{white}{\text{Line Breaker hack!!}}$  

\newpage
# Results    

Definitely need to write some text here about the results.    


```{r RMSE-summary-8, echo=FALSE}
rmse_results %>% 
  kbl(caption = "Overall Results" ) %>%
  kable_styling(full_width = FALSE, position="left")
```





\newpage
# Conclusion
Through the gradual increase in complexity of the model, the Regularized Movie and User Effect model produced an improvement of nearly 19% over the original model.

I believe with more work using the regularized techniques being applied to the Time and Genre effects, the overall model could be further improved.

Running the analysis on a relatively old PC also caused a few issues and for the further work (suggested below) it would warrant upgrading to a more powerful machine.

I would like to take the opportunity to thank the staff and other students at HarvardX for producing a very enjoyable, stimulating and often challenging course. In particular I would like to thank Prof. Irizarry for his knowledge and clarity with the online videos and assessment material.

Whilst I was aware of the topics of r, probability and regression from my former studies it has been many years since I have used them and the topics of wrangling and machine learning were completely new to me. They were topics that I thoroughly enjoyed learning albeit at a level that I know is just scratching the surface. I believe my data science journey has just begun...

(Un)fortunately due to my current job situation, I was able to complete the course in a shorter than recommended period. I believe this was ultimately very beneficial to me and allowed me to keep a high level of focus. I'm not sure I would have managed to study so hard and maintain a full time job at the same time had the situation occurred!!


## Limitations
Small data set

## Future Work
Whilst the above analysis has already produced a lot of insights and a meaningful model, further investigations could be carried out along the following lines:   

* Find a better way to pick different variables - correlations
* Extend the data to other areas of the country
* Increase data size
* Speak with real estate agents to see if there is other data that could/should be incorporated/ignored
* Real Estate special sauce
  + Add something here....   
  + And here...
    - What's happening here??
    - And here ???
* Go back further in time

* The following points relate to the actual analysis that is performed:   
  + Factor analysis   
    - Repeat analysis with a larger dataset - PC already blows up with a relatively small data set   
    - Use the recommenderlab package to extend the type of analysis performed   
  + Explore other (currently) unknown models/techniques 
  + Ensemble model
  + PCA
  + Neural network - look at the tutorial




\newpage
\appendix
# Appendix   
The following sections provide some background to the analysis that was carried out for this report.   

## Boruta Importance Analysis
Re: Boruta: this paper  https://www.jstatsoft.org/index.php/jss/article/view/v036i11/v36i11.pdf    explains the underlying algorithm. This is a high-level outline of the algorithm:     
1) A copy is made of each explanatory variable. The values in these copies are permuted to remove any correlation with the target variable. The copies are called Shadow variables.     
2) A random forest model is fitted on this expanded data set.     
3) For each variable (the original and Shadow) the Z-score of the loss in accuracy is calculated. The Z-score is the average loss divided by the standard deviation.     
4) Keep track of the original attributes z-score that score higher than the maximum of z-score of the shadow attributes.     

Repeat the above steps numerous times. Original attributes that are significantly--statistically--higher then the maximum z-score of shadow attributes are deemed relevant to the prediction. Attributes that are significantly below the maximum z-score of shadow attributes are deemed not relevant.

## LASSO Regression
Explain what it is

https://en.wikipedia.org/wiki/Lasso_(statistics)

https://www.statisticshowto.com/lasso-regression/


Page 159 and 278 of oreilly

## Random Forest
Explain what a random forest is

https://en.wikipedia.org/wiki/Random_forest

https://builtin.com/data-science/random-forest-algorithm

Page 237 of the Practical Stats book - look at bagging and the key ideas


## XGBoost
https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/

https://en.wikipedia.org/wiki/XGBoost

270 of oreilly

Explain what a XGBoost is
https://xgboost.readthedocs.io/en/latest/index.html
https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html ->>>>> good intro


xgb.train: eXtreme Gradient Boosting Training
https://www.rdocumentation.org/packages/xgboost/versions/1.4.1.1/topics/xgb.train

## Cross Validation
### Background     
A common goal of machine learning is to find an algorithm that produces predictors $\hat{Y}$ of an outcome $Y$ that minimizes the MSE:
  
$$MSE = E \left\{ \frac{1}{N} \sum_{i=i}^{N} (\hat{Y}_i - Y_i)^2 \right\}$$

When there is only one dataset, it is possible to estimate the MSE with the observed MSE like this:

$$\hat{MSE} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2$$

These two are referred to as the true error and apparent error, respectively.

There are two important characteristics of the apparent error that should always be kept in mind:

* Because the data is random, the apparent error is a random variable. For example, the given dataset may be a random sample from a larger population. An algorithm may have a lower apparent error than another algorithm due to luck.

* If an algorithm is trained on the same dataset that is used to compute the apparent error, it might result in overtraining. In general, when this action is performed, the apparent error will be an underestimate of the true error.

Cross-validation is a technique that permits the alleviation of both these problems. To understand cross-validation, it helps to think of the true error, a theoretical quantity, as the average of many apparent errors obtained by applying the algorithm to $B$ new random samples of the data, none of them used to train the algorithm. The true error can be thought of as:

$$\frac{1}{B} \sum_{b=1}^{B} \frac{1}{N}  \sum_{i=i}^{N} (\hat{y}_{i}^{b} - y_{i}^{b})^2  $$

with $B$ a large number that can be thought of as practically infinite. As already mentioned, this is a theoretical quantity because there is only one available set of outcomes: $y_1, \dots ,y_n$ . Cross validation is based on the idea of imitating the theoretical setup above as best as possible with the available data. To do this, it is necessary to generate a series of different random samples. There are several approaches that can be employed, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error.

For most machine learning algorithms it is necessary to select parameters, let's say $\lambda$, that minimises the value of MSE. It is known that if optimsation and evaluation take place on the training set, overtraining will occur. It is also the golden rule of machine learning to not use the original validation data set and this is where cross-validation is most useful.

### K-fold cross-validation    
One of the possibilities (and the one used in the regularization part of this project) is K-fold cross-validation. This technique divides the data into $K$ subsets (folds) of almost equal size. Out of these $K$ folds, one subset is used as a validation set and the others are involved in training the model.

The following describes the complete working procedure of this method:

1. Split the training dataset randomly into K subsets

2. Use K-1 subsets for the training of the model

3. Test the model against that one subset which was left out in the previous step: 

$$\hat{MSE_{b}}(\lambda) = \frac{1}{M}  \sum_{i=1}^{M} (\hat{y}_{i}^{b}(\lambda) - y_{i}^{b})^2 $$

4. Repeat the above steps $K$ times i.e. until the model is trained and tested on all subsets. This generates: 

$$\hat{MSE_{1}}(\lambda) , \hat{MSE_{2}}(\lambda) , \dots , \hat{MSE_{K}}(\lambda)$$

5. Generate an overall prediction error by taking the average of prediction errors in every case:

$$\hat{MSE}(\lambda) = \frac{1}{B} \sum_{b=1}^{K} \hat{MSE_{b}}(\lambda)$$

6. The above method describes one iteration of the process with one value of the tuning parameter $\lambda$ being fixed. In general, the above is repeated with several different values of the tuning parameter to find the optimal value of $\lambda$ that minimises the MSE.

7. Fit the final model with the obtained optimal tuning parameter(s).

8. Finally test the results from the previous step on the original independent validation dataset using the final model. 

In this project, it was decided to use a value of 5 for $K$. Large values of $K$ are preferable because the training data better imitates the original dataset. However, larger values of $K$ will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of $K=5$ and $K=10$ are popular.

### Other techniques and implementations     
One way to improve the variance of the final estimate is to take more samples. To do this, it would no longer require the training set to be partitioned into non-overlapping sets. Instead, just pick $K$ sets of some size at random.

One version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach has some advantages and is generally referred to as the bootstrap.   

XGB already used cross validation.




## Data Source
Update with real sources    

Details of the data set used in this report:   
https://grouplens.org/datasets/movielens/10m/

Details of the institute providing the dataset:   
https://grouplens.org/

## References
Link to wikipedia???

Galwey, N. (2014). Introduction to mixed modelling: Beyond regression and   analysis of variance.  

Irizarry, R. A. (2020). Introduction to data science: Data analysis and    prediction algorithms with R. https://doi.org/10.1201/9780429341830

Viswanathan, V. (2015). R data analysis cookbook: Over 80 recipes to help you breeze through your data analysis projects using R. 

https://en.wikipedia.org/wiki/Random_forest
https://builtin.com/data-science/random-forest-algorithm


## Course Material
Links to edX and HarvardX:   
https://www.edx.org    
https://www.edx.org/school/harvardx

Link to this course (including some of the theory quoted above):   
https://www.edx.org/professional-certificate/harvardx-data-science


